{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kdb/anaconda3/envs/virt1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.set_seed at 0x7fe03fbad670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from Model.Vanila_UNet import VanilaUNet\n",
    "from Model.BatchNormalized_ver import BN_VanilaUNet\n",
    "from Dataset import ISBI\n",
    "from utils import Random_processing\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class set_seed:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    \n",
    "    def forward(self):\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "\n",
    "args = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'max_epochs' : 50,\n",
    "    'train_batch_size': 4,\n",
    "    'valid_batch_size': 4,\n",
    "    'init_lr': 1e-2,\n",
    "    'model_name': 'bn_vanilaUnet',\n",
    "}\n",
    "\n",
    "\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mduriankim\u001b[0m (\u001b[33mdurian\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kdb/Workspace/Task/[Seg]ISBI_2012/wandb/run-20240515_131745-mgan6gpt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd/runs/mgan6gpt' target=\"_blank\">scarlet-vortex-13</a></strong> to <a href='https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd' target=\"_blank\">https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd/runs/mgan6gpt' target=\"_blank\">https://wandb.ai/durian/ISBI%20Semantic%20Segmentation-2nd/runs/mgan6gpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='ISBI Semantic Segmentation-2nd')\n",
    "wandb.run.name = args['model_name']\n",
    "wandb.run.save()\n",
    "\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset, DataLoader and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ISBI('train', Random_processing())\n",
    "ds_valid = ISBI('valid', Random_processing())\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=args['train_batch_size'], shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=args['valid_batch_size'], shuffle=False)\n",
    "\n",
    "model = VanilaUNet(in_channels=1, out_channels=2).to(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weights=None, smooth=1e-7):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth = smooth\n",
    "        if weights is None:\n",
    "            self.weights = torch.ones(num_classes)\n",
    "        else:\n",
    "            self.weights = weights\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.softmax(inputs, dim=1)\n",
    "        targets = F.one_hot(targets.squeeze().long(), num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (inputs * targets).sum((2, 3))\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum((2, 3)) + targets.sum((2, 3)) + self.smooth)\n",
    "        \n",
    "        class_weights = self.weights.to(inputs.device)\n",
    "        class_weights = class_weights.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "        weighted_dice = dice * class_weights\n",
    "        \n",
    "        return 1 - torch.mean(weighted_dice)\n",
    "\n",
    "\n",
    "class ce_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ce_loss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        targets = targets.squeeze().long()\n",
    "        loss = loss_fn(preds, targets)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# loss_fn = ce_loss()\n",
    "loss_fn = DiceLoss(num_classes=2, weights=torch.tensor([0.7, 0.3]))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = args['init_lr'], momentum=.99)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create log path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = f\"Model/{args['model_name']}/log\"\n",
    "best_model_path = log_path + '/best_model'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(best_model_path)\n",
    "\n",
    "# Set test image\n",
    "test_sample = next(iter(dl_valid))\n",
    "test_imgs, test_lbls, test_oris = test_sample['image'], test_sample['label'], test_sample['origin']\n",
    "view_img = test_oris[0].permute(1,2,0)\n",
    "view_img = ((view_img * .1662)+ .491)*255.0\n",
    "view_img = view_img.to(torch.int).detach().cpu().numpy()\n",
    "view_lbl = test_lbls[0].permute(1,2,0)\n",
    "test_imgs = test_imgs.to(args['device'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train & validation code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "      Epoch : 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class engine():\n",
    "    def model_train_one_epoch(model, samples, optimizer, loss_fn):\n",
    "        imgs, lbls = samples['image'], samples['label']\n",
    "        model.train()\n",
    "\n",
    "        imgs = imgs.to(args['device'])\n",
    "        lbls = lbls.to(args['device'])\n",
    "        preds = model(imgs)\n",
    "\n",
    "        loss = loss_fn(preds, lbls)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def model_valid_one_epoch(model, samples, loss_fn):\n",
    "        imgs, lbls = samples['image'], samples['label']\n",
    "        model.eval()\n",
    "\n",
    "        imgs = imgs.to(args['device'])\n",
    "        lbls = lbls.to(args['device'])\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        loss = loss_fn(preds, lbls)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "def run():\n",
    "    best_val_loss = 1e9\n",
    "\n",
    "    for epoch in range(args['max_epochs']):\n",
    "        print('--------------------------------')\n",
    "        print(f'      Epoch : {epoch}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        for _, data in tqdm(enumerate(dl_train), total=len(dl_train)):\n",
    "            train_loss= engine.model_train_one_epoch(model, data, optimizer, loss_fn)\n",
    "        \n",
    "        for _, data in tqdm(enumerate(dl_valid), total=len(dl_valid)):\n",
    "            valid_loss = engine.model_valid_one_epoch(model, data, loss_fn)\n",
    "\n",
    "        \n",
    "        # torch.save(model.state_dict(), os.path.join(log_path, f'epoch_{epoch}.pth'))\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(best_model_path, 'best_model.pth'))\n",
    "            best_val_loss = valid_loss\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            view_preds = model(test_imgs)\n",
    "            view_preds = torch.argmax(view_preds, dim=1)\n",
    "            view_pred = view_preds[0].to(torch.int)\n",
    "            view_pred = view_pred.detach().cpu()\n",
    "\n",
    "        \n",
    "        wandb.log(\n",
    "            {\n",
    "                'epoch' : epoch,\n",
    "                'train loss' : train_loss,\n",
    "                'valid_loss' : valid_loss,\n",
    "            }\n",
    "        )\n",
    "        # Save Inference\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(view_img)\n",
    "        plt.title('origin tile')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(view_lbl)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(view_pred)\n",
    "        plt.title('Prediction')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.savefig(os.path.join(log_path, f'Inference_{epoch}'))\n",
    "        plt.close()\n",
    "        print()\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
